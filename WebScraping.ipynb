{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPeoDcTjxs7C2LCdCHk0ys9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ananyaluthra/Web-Scraping/blob/main/WebScraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zq6ffUmLC1k",
        "outputId": "6b793b1a-da7d-4d70-e8ec-1d1ae0a19b3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bs4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIdcg5DfPqLS",
        "outputId": "3c1362f2-cbfa-4597-8869-08440c07c474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bs4\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.5)\n",
            "Building wheels for collected packages: bs4\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1256 sha256=50c49eb69cb0e084b03668141d96b989fcddd47bee28c7e3ced808966130520c\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
            "Successfully built bs4\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import bs4\n",
        "from pathlib import Path\n",
        "import json\n",
        "import argparse"
      ],
      "metadata": {
        "id": "Z0lkDPc4LVY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data():\n",
        "  headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "  }\n",
        "  file=Path('/content/new.html')\n",
        "  if not file.exists():\n",
        "    data = requests.get(\"https://www.montrealgazette.com\", headers=headers)\n",
        "    with open('new.html','w') as f:\n",
        "      f.write(data.text)\n",
        "  else:\n",
        "    with open('new.html','r') as f:\n",
        "      return f.read()"
      ],
      "metadata": {
        "id": "PsCW0x1ILaQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def open_links(j,link):\n",
        "  names=['First','Second','Third','Fourth','Fifth']\n",
        "  new_file=names[j]+''+'.html'\n",
        "  path='/content/'+new_file\n",
        "  headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "  }\n",
        "  file=Path(path)\n",
        "  if not file.exists():\n",
        "    data = requests.get(link, headers=headers)\n",
        "    with open(new_file,'w') as f:\n",
        "      f.write(data.text)\n",
        "  else:\n",
        "    with open(new_file,'r') as f:\n",
        "      return f.read()\n"
      ],
      "metadata": {
        "id": "feWhyoPuwEkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def output(Output,Title,Subtitle,Author,PublishDate,para):\n",
        "  final={'Title':Title,'Publication Date':PublishDate,'Author':Author,'Blurb':Subtitle}\n",
        "  Output.append(final)\n",
        "  Output.append(para)\n",
        "  return Output\n",
        "\n"
      ],
      "metadata": {
        "id": "1KBL3h4pMSwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  parser=argparse.ArgumentParser()\n",
        "  parser.add_argument('-o','--output',required=True,help=\"Output File\")\n",
        "  args=parser.parse_args()\n",
        "  links=[]\n",
        "  para=[]\n",
        "  html_data=get_data()\n",
        "  soup=bs4.BeautifulSoup(html_data,\"html.parser\")\n",
        "  articles=soup.find_all('article',class_=\"article-card article-card--image-left article-card--hide-padlock\")\n",
        "  if articles:\n",
        "    for i in range(len(articles)):\n",
        "      data=json.loads(articles[i]['data-evt-val'])\n",
        "      links.append(data['target_url'])\n",
        "  j=0\n",
        "  Final_list=[]\n",
        "  for i in links:\n",
        "    webpage_data=open_links(j,i)\n",
        "    j+=1\n",
        "    soup2=bs4.BeautifulSoup(webpage_data,\"html.parser\")\n",
        "    divs=soup2.find('div',class_='article-header__detail__texts')\n",
        "    if divs:\n",
        "      Title=divs.find('h1',class_='article-title')\n",
        "      Subtitle=divs.find('p',class_='article-subtitle')\n",
        "      Author=divs.find('a')\n",
        "      Date=divs.find('div',class_='published-date')\n",
        "      if Date:\n",
        "        PublishDate=Date.find('span',class_='published-date__since')\n",
        "    text=soup2.find_all('section',class_=\"article-content__content-group article-content__content-group--story\")\n",
        "    if text:\n",
        "      for i in text:\n",
        "        article_info=i.find_all('p')\n",
        "    if(article_info):\n",
        "      para.append(article_info)\n",
        "    result=output(Final_list,Title.text,Subtitle.text,Author.text,PublishDate.text,para)\n",
        "    with open(args.output,'w') as filewrite:\n",
        "      json.dump(result,filewrite)\n"
      ],
      "metadata": {
        "id": "Y2zZhhyTL40V"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VtVR9wd7-08M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ze-Cp0v7P_C4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}